\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{tikz-cd}
\usepackage[margin=2.5cm]{geometry}

\newtheorem{theorem}{Theorem}
\newtheorem{question}{Question}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}

\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\iss}[1]{\mathcal{#1}_S}
\newcommand{\isc}[1]{\mathcal{#1}_C}
\newcommand{\isr}[1]{\mathcal{#1}_R}
\newcommand{\ist}[1]{\mathcal{#1}_t}

\DeclareMathOperator{\Fam}{Fam}

\title{\vspace{-2cm}Interaction Structures}

\begin{document}
    \maketitle

    In this document, we will present interaction structures and show their role in our development.  We use, amongst
    other things, the work of Hancock (Ordinals and Interactive Programs).  Our development is phrased in type
    theoretical term terms, with care taken to avoid the use of the law of excluded middle, and so we expect this to be
    formalisable in Agda in a straightforward manner.  We will use $\mathbf{Type}$ to denote the category of types and
    functions in which we perform most of these constructions.  For some constructions, we will need to go to higher
    categories $\mathbf{Type}_1, \mathbf{Type}_2, \ldots$; as much as possible, we will leave this implicit.

    When talking of a family of types $B$ indexed over $A$, we will use $B$ to refer both to the family and to the
    coproduct $\sum_{a \in A} B[a]$.  Essentially, this corresponds to the view that an indexed family is a type $B$
    together with a projection $B \to A$.

    \section{Interaction Structures}

    \begin{definition}
        An \emph{interaction structure} is a tuple $(S, C, R, t)$, where $S$ is a type, $C$ is an $S$-indexed family of
        types, $R$ is a $C$-indexed family of types, and $t$ is a transition function $R \to S$.
    \end{definition}

    Note that $R$ has an implicit dependency on $S$ (corresponding to the composition of the projections).  Given an
    interaction structure $\mc I$, we denote the components by $\iss I, \isc I$, and so forth.

    According to Hancock, an equivalent presentation exists as follows: let $\Fam(X)$ be the type of functions into $X$,
    indexed by their domain. An interaction structure $(S, C, R, t)$ corresponds to a function $S \to \Fam^2(S)$.
    Writing this out, the type of interaction structures corresponds to the type
    \[
        \sum_S (S \to (\sum_C (C \to \sum_R (R \to S)))).
    \]

    The transformation is given as follows:
    \[
        (S, \lambda s.\, (C[s], \lambda c.\, (R[c], \lambda r.\, t(r)))).
    \]
    We thus see that an interaction structure is a coalgebra on the functor $\Fam^2$.

    \subsection{Coalgebra Morphisms}

    Since coalgebras have a morphism
    structure, we can consider interaction structures to form a category.  Explicitly, a morphism from a coalgebra $\phi
    : S \to \Fam^2(S)$ to a coalgebra $\psi : S' \to \Fam^2(S')$ is a function $f : S \to S'$ such that the following
    diagram commutes:

    \begin{center}
        \begin{tikzcd}
            S \arrow[rr, "f"] \arrow[d, "\phi"] && S' \arrow[d, "\psi"] \\
            \Fam^2(S) \arrow[rr, "\Fam^2(f)"] && \Fam^2(S')
        \end{tikzcd}
    \end{center}

    Note that the functorial structure on $\Fam$ acts by postcomposition on the second component: a map $X \to Y$ is
    lifted to a map $\sum_I (I \to X) \to \sum_I (I \to Y)$.  The condition thus states that for any $s : S$, the
    commands of $\phi$ at $s$ are equal to the commands of $\psi$ at $f(s)$, the responses are equal for every command,
    and the next states commute with $f$.  This, unfortunately, is too restrictive to work as a set of morphisms for our
    purposes.

    \subsection{Interaction Structure Morphisms}

    Given interaction structures $\mc I$ and $\mc J$ a morphism $\alpha : \mc I \to \mc J$ represents the ability to
    simulate the interactions provided by $\mathcal{I}$ by interactions provided by $\mc J$.  In other words, every
    command $c : \isc I$ must have a corresponding command $\alpha(c) : \isc J$, and every response of $r : \isr
    J[\alpha(c)]$ must have a corresponding response $\alpha(r) : \isr I [c]$.

    The interesting question is how the state types $\iss I$ and $\iss J$ must relate to each other.  We present two
    possible approaches: when we require a function $\iss I \to \iss J$ and when we require a relation between $\iss I$
    and $\iss J$.

    \subsubsection{Functional Morphisms}

    \begin{definition}
        A \emph{functional morphism} $\mc I \to \mc J$ is a tuple $(\alpha_S, \alpha_C, \alpha_R)$, where $\alpha_S :
        \iss I \to \iss J$, for every $s : \iss I$, $\alpha_C : \isc I [s] \to \isc J [\alpha_S(s)]$, and for every $c :
        \isc I [s]$, $\alpha_R : \isr J [\alpha_C(c)] \to \isc I [c]$, such that the following diagram commutes for
        every $s : \iss I$ and every $c : \isc I [s]$:
        \begin{center}
            \begin{tikzcd}
                \isr I [c] \arrow[d, "t"] && \isr J [\alpha_C(c)] \arrow[ll, "\alpha_R"] \arrow[d, "t'"] \\
                \iss I \arrow[rr, "\alpha_S"] && \iss J
            \end{tikzcd}
        \end{center}
    \end{definition}

    We will often use $\alpha$ to refer to each of the components of this tuple.

    It is easy to see that every interaction structure has a corresponding identity morphism, and that the composition
    of two functional morphisms is again a functional morphism.  The laws are also easily checked, and thus interaction
    structures and functional morphisms form a category.  We will denote this category $\mathbf{IS}_F$.

    \begin{theorem}
        $(\{\star\}, \{\star\}, \emptyset, \lambda ())$ is a terminal object of $\mathbf{IS}_F$.
    \end{theorem}

    \begin{theorem}
        Given interaction structures $\mc I, \mc J$, their product is given pointwise on states and commands, and is a
        disjoint union on responses.  The transition function receives a response in $I$ or in $J$ and updates the
        corresponding component of the state tuple.
    \end{theorem}

    \begin{theorem}
        $(\emptyset, \emptyset, \emptyset, \lambda ())$ is an initial object of $\mathbf{IS}_F$.
    \end{theorem}

    \begin{theorem}
        Given interaction structures $\mc I, \mc J$, their coproduct is given pointwise on states, which extends to
        commands and responses uniquely.
    \end{theorem}

    The above constructions can be generalised to any collection of interaction structures indexed by a decidable type.
    The decidability is necessary in the product case to update the component of the state in which we received a
    response.

    \subsubsection{Relational Morphisms}

    \begin{definition}
        A \emph{relational morphism} $\mc I \to \mc J$ is a tuple $(\alpha_S, \alpha_{C, p}, \alpha_R)$
        such that $\alpha_S$ is a proof-relevant relation between $\iss I$ and $\iss J$ and for every $s : \iss I$, $s'
        : \iss J$, and proof $p : \alpha_S(s, s')$,
        \begin{enumerate}
            \item $\alpha_{C, p} : \isc I [s] \to \isc J [s']$;
            \item for every $c : \isc I [s]$, $\alpha_R : \isr J [\alpha_{C, p}(c)] \to \isr I [c]$;
            \item for every $c : \isc I [s]$ and $r : \isr J [\alpha_{C, p}(c)]$, there is a proof $\alpha_S(t(\alpha_R(r)),
                t'(r))$.
        \end{enumerate}
    \end{definition}

    It is easy to see that every interaction structure has a corresponding identity morphism.  The following lemma
    guarantees that compositions also exist.

    \begin{lemma}
        Let $\mathcal{I}$, $\mathcal{J}$, $\mathcal{K}$ be interaction structures and let $\alpha : \mathcal{I} \to
        \mathcal{J}$ and $\beta : \mathcal{J} \to \mathcal{K}$.  There exists a composed morphism $\beta \circ \alpha :
        \mathcal{I} \to \mathcal{K}$.
    \end{lemma}

    \begin{proof}
        The essential component for this construction is the proof-relevance of the relation on $S$.  We define
        \[
            (\beta \circ \alpha)_S(s, s'') = \sum_{s' : \iss J} \alpha_S(s, s') \times \beta_S(s', s'').
        \]

        Now given $s : \iss I$, $s'' : \iss K$, and a proof $p = (s', p_\alpha, p\beta) : (\beta \circ \alpha)_S(s, s'')$,
        we can define
        \[
            (\beta \circ \alpha)_{C, p} = \beta_{C, p_\beta} \circ \alpha_{C, p_\alpha}.
        \]

        The action on responses is given by composition with no further complications.  It remains to show that the
        morphism is coherent with the transition function.  Given $s$, $s''$, and $p$ as above, for any $c : \isc I[s]$
        and $r'' : \isr K{(\beta \circ \alpha)_{C, p}(c)}$, we can find a proof $q_\beta : \beta_S(\ist J(\beta_R(r'')),
        \ist K(r''))$ and, since $\beta_R(r'') : \isr J[\alpha_{R, p_\alpha}(c)]$ a proof $q_\alpha :
        \alpha_S(\ist I(\alpha_R(\beta_R(r''))), \ist J(\beta_R(r'')))$, which together give a proof of $(\beta \circ
        \alpha)_S(\ist I(\alpha_R(\beta_R(r''))), \ist K(r''))$.
    \end{proof}

    This construction thus gives rise to a category of interaction structures and relational morphisms, which we will
    denote $\mathbf{IS}_R$.

    \section{Indexed Monads}

    Given a type $S$ we can regard it as a discrete category to obtain the category of functions $S \to \mathbf{Type}$.
    The objects of this category are $S$-indexed collections of types, and the morphisms are $S$-indexed collections of
    functions.  We can construct this category in $\mathbf{Type}_1$.

    \begin{definition}
        An \emph{indexed monad}, indexed over a type $S$, is a monad $M$ on the category $S \to \mathbf{Type}$.
        Explicitly, it is an endofunctor on $S \to \mathbf{Type}$ with associated natural transformations $\eta :
        \mathbf{1} \to M$ and $\mu : M \circ M \to M$, satisfying the usual monad laws.
    \end{definition}

    Indexed monads form a category in a number of ways.  

    \section{Free Monads of Interaction Structures}

    It's an interesting question what makes `Free Monads' free; what functor are they the right adjoint of?
    It seems like their key property is that any implementation factors through them.  (Can this be shown formally?
    Probably should be doable.)  This gives some sort of notion of `universal arrows'.

    % This blog post describes the categorical motivation behind how Haskell does it:
    % https://www.paolocapriotti.com/blog/2013/12/04/free-monads-part-2/

\end{document}
